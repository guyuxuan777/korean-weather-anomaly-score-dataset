{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "import torch\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.sod  import SOD\n",
    "from pyod.models.rod import ROD\n",
    "import sys\n",
    "from embedding import Graph2Vec\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap, shiftgrid, maskoceans, interp\n",
    "os.chdir(\"/Users/mikegu/Desktop/weather data exploration/jiuji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(file,area):\n",
    "    Time = pd.read_csv(file)\n",
    "    Area = Time[Time['지점'] == area]\n",
    "    Area = Area.dropna(axis = 0)\n",
    "    return Area\n",
    "\n",
    "#i=1\n",
    "def data1(area):\n",
    "    \n",
    "    DF10 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2010.csv',area)\n",
    "    DF11 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2011.csv',area)\n",
    "    DF12 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2012.csv',area)\n",
    "    DF13 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2013.csv',area)\n",
    "    DF14 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2014.csv',area)\n",
    "    DF15 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2015.csv',area)\n",
    "    DF16 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2016.csv',area)\n",
    "    DF17 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2017.csv',area)\n",
    "    DF18 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2018.csv',area)\n",
    "    DF19 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2019.csv',area)\n",
    "    DF20 = data('/Users/mikegu/Desktop/weather data exploration/koreaweather/OBS_ASOS_TIM_2020.csv',area)\n",
    "    df = pd.concat([DF10,DF11,DF12,DF13,DF14,DF15,DF16,DF17,DF18,DF19,DF20])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chengshizidian = {\n",
    "106: 'Donghae',\n",
    " 108: 'Seoul',\n",
    " 112: 'Incheon',\n",
    " 114: 'Wonju',\n",
    " 115: 'Ulleung do',\n",
    " 119: 'Suwon',\n",
    " 121: 'Yeongwol',\n",
    " 127: 'Chungju',\n",
    " 129: 'Seosan',\n",
    " 130: 'Uljin',\n",
    " 131: 'Cheongju',\n",
    " 133: 'Daejeon',\n",
    " 135: 'Chupungnyeong',\n",
    " 136: 'Andong',\n",
    " 137: 'Sangju',\n",
    " 138: 'Pohang',\n",
    " 140: 'Gunsan',\n",
    " 143: 'Daegu',\n",
    " 146: 'Jeonju',\n",
    " 152: 'Ulsan',\n",
    " 155: 'Changwon',\n",
    " 156: 'Gwangju',\n",
    " 159: 'Busan',\n",
    " 162: 'Tongyeong',\n",
    " 165: 'Mokpo',\n",
    " 168: 'Yeosu',\n",
    " 170: 'Wando',\n",
    " 184: 'Jeju',\n",
    " 189: 'Seogwipo',\n",
    " 192: 'Jinju',\n",
    " 201: 'Ganghwa',\n",
    " 202: 'Yangpyeong',\n",
    " 203: 'Icheon',\n",
    " 211: 'Inje',\n",
    " 212: 'Hongcheon',\n",
    " 216: 'Taebaek',\n",
    " 221: 'Jecheon',\n",
    " 226: 'Boeun',\n",
    " 235: 'Boryeong',\n",
    " 236: 'Buyeo',\n",
    " 238: 'Geumsan',\n",
    " 243: 'Buan',\n",
    " 244: 'Imsil',\n",
    " 245: 'Jeongeup',\n",
    " 247: 'Namwon',\n",
    " 248: 'Jangsu',\n",
    " 254: 'Sunchang',\n",
    " 257: 'Yangsan',\n",
    " 259: 'Gangjin',\n",
    " 260: 'Jangheung',\n",
    " 261: 'Haenam',\n",
    " 262: 'Goheung',\n",
    " 271: 'Bonghwa',\n",
    " 272: 'Yeongju',\n",
    " 273: 'Mungyeong',\n",
    " 277: 'Yeongdeok',\n",
    " 278: 'Uiseong',\n",
    " 279: 'Gumi',\n",
    " 281: 'Yeongcheon',\n",
    " 284: 'Geochang',\n",
    " 288: 'Miryang',\n",
    " 289: 'Sancheong',\n",
    " 294: 'Geoje',\n",
    " 295: 'Namhae'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chengshizidian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
      "/var/folders/3r/stx0z6fn3y7gn9nz8mwg1xfr0000gn/T/ipykernel_60142/3748958021.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "for key,value in chengshizidian.items():    \n",
    "    df=data1(int(key))\n",
    "    #df.to_csv(\"/Users/mikegu/Desktop/weather data exploration/zhongjitest/test.csv\",encoding=\"utf-8-sig\")\n",
    "    df1=df.fillna(method=\"ffill\")\n",
    "    riqi=np.array(df1['일시'].values)\n",
    "    riqilist=riqi.tolist()\n",
    "    start='2010/1/1'\n",
    "    end='2020/12/31'\n",
    "    datestart=datetime.datetime.strptime(start,'%Y/%m/%d')\n",
    "    dateend=datetime.datetime.strptime(end,'%Y/%m/%d')\n",
    "\n",
    "    \n",
    "    data_list = list()\n",
    "    while datestart<=dateend:\n",
    "        data_list.append(datestart.strftime(\"%Y/%-m/%-d\")) \n",
    "        datestart+=datetime.timedelta(days=1)\n",
    "    data_list[0]\n",
    "    filllist = []\n",
    "    for i in data_list:\n",
    "        for j in range(0,24):\n",
    "            times = i+\" \"+str(j)+\":00\"\n",
    "            if times in riqilist:\n",
    "                #print(\"ture\")\n",
    "                filllist.append(\"true\")\n",
    "            else:\n",
    "                #print(times)\n",
    "                filllist.append(times)\n",
    "    for i, line in enumerate(filllist):\n",
    "        if line != 'true':\n",
    "            d= [[] * 12]\n",
    "            pd_arr1 = df1[:i]\n",
    "            pd_arr2 = df1[i:]\n",
    "            df1 = pd_arr1.append(d, ignore_index = True).append(pd_arr2, ignore_index=True)\n",
    "    df2 = df1.interpolate(method='linear', limit_direction='forward')\n",
    "    df2 = df2.fillna(method=\"bfill\")\n",
    "    seoul1017 = df2.iloc[:,3:]\n",
    "    tianqidf = df2[\"기온(°C)\"]\n",
    "    shidudf = df2['습도(%)']\n",
    "    zhengqiyadf = df2['증기압(hPa)']\n",
    "    ludianwendudf = df2['이슬점온도(°C)']\n",
    "    dangdiqiyadf = df2['현지기압(hPa)']\n",
    "    haimianqiyadf = df2['해면기압(hPa)']\n",
    "    dangdiwendudf = df2['지면온도(°C)']\n",
    "    tianqi=tianqidf.values \n",
    "    shidu=shidudf.values\n",
    "    zhengqiya=zhengqiyadf.values\n",
    "    ludianwendu=ludianwendudf.values\n",
    "    dangdiqiya=dangdiqiyadf.values\n",
    "    haimianqiya=haimianqiyadf.values\n",
    "    dangdiwendu=dangdiwendudf.values\n",
    "    Tianqi=[]\n",
    "    for i in range(int(len(tianqi)/24)):\n",
    "        sum1=0\n",
    "        for j in range(24):\n",
    "            sum1=sum1+tianqi[j+(i*24)]\n",
    "        Tianqi.append(sum1/24)\n",
    "    tianqi1=np.array(Tianqi)  \n",
    "    tianqi2=np.around(tianqi1,5)  \n",
    "    Shidu=[]\n",
    "    for i in range(int(len(shidu)/24)):\n",
    "        sum2=0\n",
    "        for j in range(24):\n",
    "            sum2=sum2+shidu[j+(i*24)]\n",
    "        Shidu.append(sum2/24)\n",
    "    shidu1=np.array(Shidu)  \n",
    "    shidu2=np.around(shidu1,5) \n",
    "    Zhengqiya=[]\n",
    "    for i in range(int(len(zhengqiya)/24)):\n",
    "        sum3=0\n",
    "        for j in range(24):\n",
    "            sum3=sum3+zhengqiya[j+(i*24)]\n",
    "        Zhengqiya.append(sum3/24)\n",
    "    zhengqiya1=np.array(Zhengqiya)\n",
    "    zhengqiya2=np.around(zhengqiya1,5)\n",
    "    Ludianwendu=[]\n",
    "    for i in range(int(len(ludianwendu)/24)):\n",
    "        sum4=0\n",
    "        for j in range(24):\n",
    "            sum4=sum4+ludianwendu[j+(i*24)]\n",
    "        Ludianwendu.append(sum4/24)\n",
    "    ludianwendu1=np.array(Ludianwendu)\n",
    "    ludianwendu2=np.around(ludianwendu1,5)\n",
    "    Dangdiqiya=[]\n",
    "    for i in range(int(len(dangdiqiya)/24)):\n",
    "        sum5=0\n",
    "        for j in range(24):\n",
    "            sum5=sum5+dangdiqiya[j+(i*24)]\n",
    "        Dangdiqiya.append(sum5/24)\n",
    "    dangdiqiya1=np.array(Dangdiqiya)\n",
    "    dangdiqiya2=np.around(dangdiqiya1,5)\n",
    "    Haimianqiya=[]\n",
    "    for i in range(int(len(haimianqiya)/24)):\n",
    "        sum6=0\n",
    "        for j in range(24):\n",
    "            sum6=sum6+haimianqiya[j+(i*24)]\n",
    "        Haimianqiya.append(sum6/24)\n",
    "    haimianqiya1=np.array(Haimianqiya)\n",
    "    haimianqiya2=np.around(haimianqiya1,5)\n",
    "    Dangdiwendu=[]\n",
    "    for i in range(int(len(dangdiwendu)/24)):\n",
    "        sum7=0\n",
    "        for j in range(24):\n",
    "            sum7=sum7+dangdiwendu[j+(i*24)]\n",
    "        Dangdiwendu.append(sum7/24)\n",
    "    dangdiwendu1=np.array(Dangdiwendu)\n",
    "    dangdiwendu2=np.around(dangdiwendu1,5)\n",
    "    city=value\n",
    "    City=[]\n",
    "    for i in range(len(Tianqi)):\n",
    "        City.append(city)\n",
    "    def processingforcorr(data):\n",
    "            Corr = data.corr().values\n",
    "            Corr = np.abs(Corr)\n",
    "            Corr = pd.DataFrame(Corr)\n",
    "            return Corr.fillna(0).values\n",
    "\n",
    "    def Graph(data):\n",
    "        maxlag = 1\n",
    "        test = 'ssr_chi2test'\n",
    "        X = []\n",
    "        new = processingforcorr(data.iloc[:24])\n",
    "        X.append(new)\n",
    "        i = 1\n",
    "        while i<=data.shape[0]-25:\n",
    "            new = processingforcorr(data.iloc[i:i+24])\n",
    "            X.append(new)\n",
    "            i=i+24\n",
    "                \n",
    "        return np.array(X)\n",
    "\n",
    "    def entropy(X):\n",
    "            E = []\n",
    "            for i in range(X.shape[0]):\n",
    "                P = []\n",
    "                for j in range(X.shape[1]):\n",
    "                    if i !=j:\n",
    "                        e = -X[i][j]*np.log(X[i][j])\n",
    "                        P.append(e)\n",
    "                P = np.array(P)\n",
    "                E.append(np.sum(P))\n",
    "            return np.array(E)\n",
    "\n",
    "    def graphentropy(X):\n",
    "            E = []\n",
    "            for i in range(X.shape[0]):\n",
    "                e = entropy(X[i])\n",
    "                E.append(np.sum(e))\n",
    "            return np.array(E)\n",
    "\n",
    "    def distance(x,y):\n",
    "            distance = np.mean(np.power((x - y),2))\n",
    "            return distance\n",
    "\n",
    "    def getMatrix(data,E):\n",
    "            Matrix = []\n",
    "            for i in range(E.shape[0]):\n",
    "                dis = []\n",
    "                for j in range(len(E)):\n",
    "                    dis.append(distance(E[i],E[j]))\n",
    "                dis = np.array(dis)\n",
    "                index = np.argsort(dis)[1]\n",
    "                Matrix.append(data[index])\n",
    "                \n",
    "            return np.array(Matrix)\n",
    "    x = Graph(seoul1017)\n",
    "    X = x.reshape(-1,49)\n",
    "    pca = PCA(n_components=4*4)\n",
    "    X= pca.fit_transform(X)\n",
    "    for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i][j] == 0:\n",
    "                    X[i][j] = 1\n",
    "    X = np.abs(X.reshape(X.shape[0],4,4))\n",
    "\n",
    "\n",
    "    E = graphentropy(X)\n",
    "\n",
    "    MatrixX = getMatrix(X,E)\n",
    "\n",
    "    TrainXTensor = torch.from_numpy(X.reshape(X.shape[0], 16)).type(torch.FloatTensor)\n",
    "    TrainSTensor = torch.from_numpy(MatrixX.reshape(X.shape[0], 16)).type(torch.FloatTensor)\n",
    "            \n",
    "    model = torch.load('/Users/mikegu/Desktop/weather data exploration/Model.pth')\n",
    "    model.eval()\n",
    "    ed1, ed2, de1, de2 = model(TrainXTensor, TrainSTensor)\n",
    "    Embed = ed1.data.numpy()\n",
    "\n",
    "    df_Embed = pd.DataFrame(Embed)\n",
    "    df_Embed.columns = ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "\n",
    "    start='2010-01-01'\n",
    "    end='2020-12-31'\n",
    "    \n",
    "    datestart=datetime.datetime.strptime(start,'%Y-%m-%d')\n",
    "    dateend=datetime.datetime.strptime(end,'%Y-%m-%d')\n",
    "    \n",
    "    data_list = list()\n",
    "    while datestart<=dateend:\n",
    "        data_list.append(datestart.strftime('%Y-%m-%d')) \n",
    "        datestart+=datetime.timedelta(days=1)\n",
    "    #lof\n",
    "    lof = LocalOutlierFactor(n_neighbors=10, contamination=0.2, algorithm='auto', n_jobs=-1, novelty=True)\n",
    "    lof.fit(Embed)\n",
    "    lofscore=np.abs(lof.negative_outlier_factor_)\n",
    "    normalized_lofscore = preprocessing.normalize([lofscore])\n",
    "    normalized_lofscore = np.around(normalized_lofscore[0],5)\n",
    "    #if\n",
    "    isolation_forest = IsolationForest(n_estimators=100,contamination=0.05) \n",
    "    isolation_forest.fit(Embed)\n",
    "    Ifscore = np.around(isolation_forest.decision_function(Embed),5) \n",
    "    normalized_Ifscore = preprocessing.normalize([Ifscore])\n",
    "    normalized_Ifscore = np.around(normalized_Ifscore[0],5)\n",
    "    #copod\n",
    "    copod=COPOD()\n",
    "    copod.fit(Embed)\n",
    "    copodscore=np.around(copod.decision_scores_,5)\n",
    "    normalized_copodscore = preprocessing.normalize([copodscore])\n",
    "    normalized_copodscore = np.around(normalized_copodscore[0],5)\n",
    "    #abod\n",
    "    abod=ABOD(contamination=0.1,method='fast',n_neighbors=20)\n",
    "    abod.fit(Embed)\n",
    "    abodscore=np.abs(abod.decision_scores_)\n",
    "    normalized_abodscore = preprocessing.normalize([abodscore])\n",
    "    normalized_abodscore = np.around(normalized_abodscore[0],5)\n",
    "    #hbos\n",
    "    hbos=HBOS()\n",
    "    hbos.fit(Embed)\n",
    "    hbosscore=hbos.decision_scores_\n",
    "    normalized_hbosscore = preprocessing.normalize([hbosscore])\n",
    "    normalized_hbosscore = np.abs(normalized_hbosscore)\n",
    "    normalized_hbosscore = np.around(normalized_hbosscore[0],5)\n",
    "    #sod\n",
    "    sod=SOD()\n",
    "    sod.fit(Embed)\n",
    "    sodscore=sod.decision_scores_\n",
    "    normalized_sodscore=preprocessing.normalize([sodscore])\n",
    "    normalized_sodscore=np.around(normalized_sodscore[0],5)\n",
    "    #rod\n",
    "    rod=ROD()\n",
    "    rod.fit(Embed)\n",
    "    rodscore=rod.decision_scores_\n",
    "    normalized_rodscore=preprocessing.normalize([rodscore])\n",
    "    normalized_rodscore=np.around(normalized_rodscore[0],5)\n",
    "    dic={'City':City,'Date':data_list,'Temperature(°C)':tianqi2,'Humidity(%)':shidu2,'Vapor pressure(hPa)':zhengqiya2,'Dew point temperature(°C)':ludianwendu2,'Local air pressure(hPa)':dangdiqiya2,'Sea surface pressure(hPa)':haimianqiya2,'Ground temperature(°C)':dangdiwendu2,'Anoamly Score(LOF)':normalized_lofscore,'Anomaly Score(IF)':normalized_Ifscore,'Anomaly Score(COPOD)':normalized_copodscore,'Anomaly Score(ABOD)':normalized_abodscore,'Anomaly Score(HBOS)':normalized_hbosscore,'Anomaly Score(SOD)':normalized_sodscore,'Anomaly Score(ROD)':normalized_rodscore}\n",
    "    finaldf = pd.DataFrame.from_dict(dic)\n",
    "    finaldf.to_csv(value+\"2010to2020score.csv\",mode='a',index=False,header=True,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bdfe23cf4ec81c74de625fb4b11091a464e5f7139b64bc1a98323f9ee61e821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
